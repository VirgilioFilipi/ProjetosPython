{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning com PyTorch\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Machine Learning utilizando o framework PyTorch. Abaixo estão os tópicos que serão abordados:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU está disponível?:  True\n",
      "Nome da GPU:  NVIDIA GeForce GTX 1650\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Configurando hiperparâmetros.\n",
    "args = {\n",
    "    'epoch_num': 200,     # Número de épocas.\n",
    "    'lr': 5e-5,           # Taxa de aprendizado.\n",
    "    'weight_decay': 5e-4, # Penalidade L2 (Regularização).\n",
    "    'num_workers': 3,     # Número de threads do dataloader.\n",
    "    'batch_size': 20,     # Tamanho do batch.\n",
    "}\n",
    "print(\"GPU está disponível?: \",torch.cuda.is_available())  # Verifica se a GPU está disponível\n",
    "print(\"Nome da GPU: \",torch.cuda.get_device_name(0))  # Obtém o nome da GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args['device'] = torch.device('cuda')\n",
    "else:\n",
    "    args['device'] = torch.device('cpu')\n",
    "\n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixando o dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-08 21:57:07--  https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘Bike-Sharing-Dataset.zip’\n",
      "\n",
      "Bike-Sharing-Datase     [ <=>                ] 273,43K  19,4KB/s    in 13s     \n",
      "\n",
      "2024-05-08 21:57:21 (21,4 KB/s) - ‘Bike-Sharing-Dataset.zip’ saved [279992]\n",
      "\n",
      "Archive:  Bike-Sharing-Dataset.zip\n",
      "  inflating: Readme.txt              \n",
      "  inflating: day.csv                 \n",
      "  inflating: hour.csv                \n"
     ]
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
    "! unzip Bike-Sharing-Dataset.zip  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17379\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('hour.csv')\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13903 3476\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12663</th>\n",
       "      <td>12664</td>\n",
       "      <td>2012-06-16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.6212</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>123</td>\n",
       "      <td>229</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>1802</td>\n",
       "      <td>2011-03-20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.3939</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.3582</td>\n",
       "      <td>58</td>\n",
       "      <td>98</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16567</th>\n",
       "      <td>16568</td>\n",
       "      <td>2012-11-28</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2239</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8817</th>\n",
       "      <td>8818</td>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>2609</td>\n",
       "      <td>2011-04-23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.5455</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.3582</td>\n",
       "      <td>182</td>\n",
       "      <td>209</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       instant      dteday  season  yr  mnth  hr  holiday  weekday  \\\n",
       "12663    12664  2012-06-16       2   1     6  20        0        6   \n",
       "1801      1802  2011-03-20       1   0     3  18        0        0   \n",
       "16567    16568  2012-11-28       4   1    11   1        0        3   \n",
       "8817      8818  2012-01-08       1   1     1   5        0        0   \n",
       "2608      2609  2011-04-23       2   0     4  14        0        6   \n",
       "\n",
       "       workingday  weathersit  temp   atemp   hum  windspeed  casual  \\\n",
       "12663           0           2  0.66  0.6212  0.47     0.1940     123   \n",
       "1801            0           1  0.38  0.3939  0.40     0.3582      58   \n",
       "16567           1           2  0.26  0.2576  0.75     0.2239       0   \n",
       "8817            0           2  0.32  0.3333  0.49     0.1045       0   \n",
       "2608            0           1  0.58  0.5455  0.78     0.3582     182   \n",
       "\n",
       "       registered  cnt  \n",
       "12663         229  352  \n",
       "1801           98  156  \n",
       "16567          12   12  \n",
       "8817            2    2  \n",
       "2608          209  391  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bike-Sharing-Dataset.zip  bike_train.csv  hour.csv\t\tReadme.txt\n",
      "bike_test.csv\t\t  day.csv\t  neural_network.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Train/Test split\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(df)).tolist()\n",
    "\n",
    "train_size = int(0.8*len(df))\n",
    "df_train = df.iloc[indices[:train_size]]\n",
    "df_test  = df.iloc[indices[train_size:]]\n",
    "\n",
    "print(len(df_train), len(df_test))\n",
    "display(df_test.head())\n",
    "\n",
    "df_train.to_csv('bike_train.csv',index=False)\n",
    "df_test.to_csv('bike_test.csv',index=False)\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bicicletinha(Dataset):\n",
    "  def __init__(self, csv_path, scaler_feat=None, scaler_label=None):\n",
    "  \n",
    "    self.dados = pd.read_csv(csv_path).to_numpy()\n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    \n",
    "    sample = self.dados[idx][2:14]\n",
    "    label  = self.dados[idx][-1:]\n",
    "    \n",
    "    # converte para tensor\n",
    "    sample = torch.from_numpy(sample.astype(np.float32))\n",
    "    label  = torch.from_numpy(label.astype(np.float32))\n",
    "    \n",
    "    return sample, label\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([373.])\n",
      "tensor([ 4.0000,  1.0000, 11.0000, 19.0000,  0.0000,  4.0000,  1.0000,  1.0000,\n",
      "         0.3800,  0.3939,  0.2700,  0.3582])\n"
     ]
    }
   ],
   "source": [
    "dataset = Bicicletinha('bike_train.csv')\n",
    "dado, rotulo = dataset[0]\n",
    "print(rotulo)\n",
    "print(dado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do treino: 13903 amostras\n",
      "Tamanho do teste: 3476 amostras\n"
     ]
    }
   ],
   "source": [
    "train_set = Bicicletinha('bike_train.csv')\n",
    "test_set  = Bicicletinha('bike_test.csv')\n",
    "\n",
    "print('Tamanho do treino: ' + str(len(train_set)) + ' amostras')\n",
    "print('Tamanho do teste: ' + str(len(test_set)) + ' amostras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando dataloader\n",
    "train_loader = DataLoader(train_set,\n",
    "                          args['batch_size'],\n",
    "                          num_workers=args['num_workers'],\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(test_set,\n",
    "                         args['batch_size'],\n",
    "                         num_workers=args['num_workers'],\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([20, 12]) torch.Size([20, 1])\n",
      "## Dimensionalidade do batch ##\n",
      "torch.Size([16, 12]) torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "  \n",
    "  dado, rotulo = batch\n",
    "  print('## Dimensionalidade do batch ##')\n",
    "  print(dado.size(), rotulo.size())\n",
    "  \n",
    "#   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (features): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "  \n",
    "  def __init__(self, input_size, hidden_size, out_size):\n",
    "    super(MLP, self).__init__()\n",
    "    \n",
    "    self.features = nn.Sequential(\n",
    "          nn.Linear(input_size, hidden_size),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(hidden_size, hidden_size),\n",
    "          nn.ReLU(),\n",
    "    )\n",
    "    \n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Linear(hidden_size, out_size),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "  def forward(self, X):\n",
    "    \n",
    "    hidden = self.features(X)\n",
    "    output = self.classifier(hidden)\n",
    "    \n",
    "    return output\n",
    "\n",
    "input_size  = train_set[0][0].size(0)\n",
    "hidden_size = 128\n",
    "out_size    = 1\n",
    "\n",
    "net = MLP(input_size, hidden_size, out_size).to(args['device'])\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss().to(args['device'])\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, epoch):\n",
    "\n",
    "  # Training mode\n",
    "  net.train()\n",
    "  \n",
    "  start = time.time()\n",
    "  \n",
    "  epoch_loss  = []\n",
    "  for batch in train_loader:\n",
    "    \n",
    "    dado, rotulo = batch\n",
    "    \n",
    "    # Cast do dado na GPU\n",
    "    dado = dado.to(args['device'])\n",
    "    rotulo = rotulo.to(args['device'])\n",
    "    \n",
    "    # Forward\n",
    "    ypred = net(dado)\n",
    "    loss = criterion(ypred, rotulo)\n",
    "    epoch_loss.append(loss.cpu().data)\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "   \n",
    "  epoch_loss = np.asarray(epoch_loss)\n",
    "  \n",
    "  end = time.time()\n",
    "  print('#################### Train ####################')\n",
    "  print('Epoch %d, Loss: %.4f +/- %.4f, Time: %.2f' % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start))\n",
    "  \n",
    "  return epoch_loss.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(test_loader, net, epoch):\n",
    "\n",
    "  # Evaluation mode\n",
    "  net.eval()\n",
    "  \n",
    "  start = time.time()\n",
    "  \n",
    "  epoch_loss  = []\n",
    "  \n",
    "  with torch.no_grad(): \n",
    "    for batch in test_loader:\n",
    "\n",
    "      dado, rotulo = batch\n",
    "\n",
    "      # Cast do dado na GPU\n",
    "      dado = dado.to(args['device'])\n",
    "      rotulo = rotulo.to(args['device'])\n",
    "\n",
    "      # Forward\n",
    "      ypred = net(dado)\n",
    "      loss = criterion(ypred, rotulo)\n",
    "      epoch_loss.append(loss.cpu().data)\n",
    "\n",
    "  epoch_loss = np.asarray(epoch_loss)\n",
    "  \n",
    "  end = time.time()\n",
    "  print('********** Validate **********')\n",
    "  print('Epoch %d, Loss: %.4f +/- %.4f, Time: %.2f\\n' % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start))\n",
    "  \n",
    "  return epoch_loss.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Train ####################\n",
      "Epoch 0, Loss: 164.3138 +/- 42.0975, Time: 1.60\n",
      "********** Validate **********\n",
      "Epoch 0, Loss: 125.7965 +/- 30.3890, Time: 0.31\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 1, Loss: 130.6107 +/- 25.6430, Time: 1.21\n",
      "********** Validate **********\n",
      "Epoch 1, Loss: 126.6351 +/- 24.3811, Time: 0.30\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 2, Loss: 122.8896 +/- 30.2196, Time: 1.32\n",
      "********** Validate **********\n",
      "Epoch 2, Loss: 125.2943 +/- 31.5335, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 3, Loss: 120.4436 +/- 31.0856, Time: 1.32\n",
      "********** Validate **********\n",
      "Epoch 3, Loss: 117.2516 +/- 26.3097, Time: 0.33\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 4, Loss: 118.8115 +/- 24.6739, Time: 1.41\n",
      "********** Validate **********\n",
      "Epoch 4, Loss: 114.5222 +/- 28.4489, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 5, Loss: 118.3620 +/- 31.7777, Time: 1.33\n",
      "********** Validate **********\n",
      "Epoch 5, Loss: 115.1214 +/- 29.4287, Time: 0.37\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 6, Loss: 115.6204 +/- 27.2289, Time: 1.37\n",
      "********** Validate **********\n",
      "Epoch 6, Loss: 114.4765 +/- 26.8682, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 7, Loss: 113.9728 +/- 27.5468, Time: 1.24\n",
      "********** Validate **********\n",
      "Epoch 7, Loss: 111.4902 +/- 28.6821, Time: 0.38\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 8, Loss: 111.6952 +/- 25.9599, Time: 1.22\n",
      "********** Validate **********\n",
      "Epoch 8, Loss: 108.0174 +/- 26.1615, Time: 0.31\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 9, Loss: 107.8571 +/- 28.5970, Time: 1.29\n",
      "********** Validate **********\n",
      "Epoch 9, Loss: 104.4417 +/- 26.1539, Time: 0.32\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 10, Loss: 103.9991 +/- 24.9886, Time: 1.28\n",
      "********** Validate **********\n",
      "Epoch 10, Loss: 100.6571 +/- 26.5733, Time: 0.32\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 11, Loss: 99.7821 +/- 24.9275, Time: 1.56\n",
      "********** Validate **********\n",
      "Epoch 11, Loss: 95.8669 +/- 24.1808, Time: 0.32\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 12, Loss: 96.6053 +/- 24.3821, Time: 1.24\n",
      "********** Validate **********\n",
      "Epoch 12, Loss: 95.3180 +/- 23.4885, Time: 0.31\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 13, Loss: 96.5437 +/- 24.5865, Time: 1.44\n",
      "********** Validate **********\n",
      "Epoch 13, Loss: 93.7652 +/- 22.9804, Time: 0.32\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 14, Loss: 94.7331 +/- 23.7863, Time: 1.16\n",
      "********** Validate **********\n",
      "Epoch 14, Loss: 90.6606 +/- 22.7362, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 15, Loss: 92.5012 +/- 22.5402, Time: 1.32\n",
      "********** Validate **********\n",
      "Epoch 15, Loss: 90.1320 +/- 24.0746, Time: 0.36\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 16, Loss: 92.0659 +/- 23.4133, Time: 1.30\n",
      "********** Validate **********\n",
      "Epoch 16, Loss: 91.1397 +/- 24.7242, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 17, Loss: 91.8916 +/- 22.9651, Time: 1.16\n",
      "********** Validate **********\n",
      "Epoch 17, Loss: 90.4353 +/- 23.9504, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 18, Loss: 91.6705 +/- 23.8103, Time: 1.18\n",
      "********** Validate **********\n",
      "Epoch 18, Loss: 88.3904 +/- 23.3149, Time: 0.26\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 19, Loss: 90.8965 +/- 22.6150, Time: 1.16\n",
      "********** Validate **********\n",
      "Epoch 19, Loss: 87.5400 +/- 21.5816, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 20, Loss: 90.7682 +/- 22.8921, Time: 1.17\n",
      "********** Validate **********\n",
      "Epoch 20, Loss: 87.4189 +/- 21.5550, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 21, Loss: 89.9812 +/- 23.7635, Time: 1.11\n",
      "********** Validate **********\n",
      "Epoch 21, Loss: 85.9650 +/- 23.2444, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 22, Loss: 89.3827 +/- 22.9388, Time: 1.15\n",
      "********** Validate **********\n",
      "Epoch 22, Loss: 88.4128 +/- 25.0879, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 23, Loss: 88.7066 +/- 23.0767, Time: 1.11\n",
      "********** Validate **********\n",
      "Epoch 23, Loss: 89.4212 +/- 25.2100, Time: 0.31\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 24, Loss: 88.0540 +/- 23.8697, Time: 1.12\n",
      "********** Validate **********\n",
      "Epoch 24, Loss: 85.1357 +/- 23.4307, Time: 0.26\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 25, Loss: 88.7124 +/- 23.8201, Time: 1.16\n",
      "********** Validate **********\n",
      "Epoch 25, Loss: 87.3676 +/- 21.4720, Time: 0.26\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 26, Loss: 87.7239 +/- 22.3883, Time: 1.16\n",
      "********** Validate **********\n",
      "Epoch 26, Loss: 90.2906 +/- 20.7736, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 27, Loss: 88.0972 +/- 23.5305, Time: 1.18\n",
      "********** Validate **********\n",
      "Epoch 27, Loss: 84.4945 +/- 21.3096, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 28, Loss: 88.5125 +/- 22.5977, Time: 1.14\n",
      "********** Validate **********\n",
      "Epoch 28, Loss: 90.1841 +/- 25.7518, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 29, Loss: 87.4708 +/- 22.2495, Time: 1.16\n",
      "********** Validate **********\n",
      "Epoch 29, Loss: 83.7335 +/- 23.7523, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 30, Loss: 88.4807 +/- 24.0738, Time: 1.13\n",
      "********** Validate **********\n",
      "Epoch 30, Loss: 94.1654 +/- 20.5585, Time: 0.30\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 31, Loss: 89.2981 +/- 23.0086, Time: 1.10\n",
      "********** Validate **********\n",
      "Epoch 31, Loss: 85.0418 +/- 24.8204, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 32, Loss: 86.9986 +/- 21.4332, Time: 1.15\n",
      "********** Validate **********\n",
      "Epoch 32, Loss: 82.1431 +/- 23.4538, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 33, Loss: 87.1696 +/- 24.7856, Time: 1.15\n",
      "********** Validate **********\n",
      "Epoch 33, Loss: 88.1370 +/- 20.3563, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 34, Loss: 87.4213 +/- 22.2181, Time: 1.12\n",
      "********** Validate **********\n",
      "Epoch 34, Loss: 88.4506 +/- 25.7261, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 35, Loss: 86.4995 +/- 22.6055, Time: 1.14\n",
      "********** Validate **********\n",
      "Epoch 35, Loss: 84.5407 +/- 20.8441, Time: 0.30\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 36, Loss: 84.5875 +/- 24.7618, Time: 1.16\n",
      "********** Validate **********\n",
      "Epoch 36, Loss: 80.2940 +/- 22.5541, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 37, Loss: 84.1822 +/- 20.7617, Time: 1.17\n",
      "********** Validate **********\n",
      "Epoch 37, Loss: 81.5659 +/- 23.4861, Time: 0.30\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 38, Loss: 84.2454 +/- 22.2239, Time: 1.19\n",
      "********** Validate **********\n",
      "Epoch 38, Loss: 85.6009 +/- 20.3368, Time: 0.26\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 39, Loss: 84.0512 +/- 22.9692, Time: 1.13\n",
      "********** Validate **********\n",
      "Epoch 39, Loss: 83.0296 +/- 24.1280, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 40, Loss: 83.1504 +/- 20.3510, Time: 1.13\n",
      "********** Validate **********\n",
      "Epoch 40, Loss: 78.8123 +/- 21.4825, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 41, Loss: 82.8517 +/- 23.3117, Time: 1.17\n",
      "********** Validate **********\n",
      "Epoch 41, Loss: 81.6388 +/- 20.3084, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 42, Loss: 83.7852 +/- 22.2821, Time: 1.14\n",
      "********** Validate **********\n",
      "Epoch 42, Loss: 84.0388 +/- 24.6801, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 43, Loss: 82.8654 +/- 21.8512, Time: 1.17\n",
      "********** Validate **********\n",
      "Epoch 43, Loss: 81.0668 +/- 20.2041, Time: 0.30\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 44, Loss: 81.2365 +/- 21.8818, Time: 1.17\n",
      "********** Validate **********\n",
      "Epoch 44, Loss: 77.5149 +/- 21.2130, Time: 0.31\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 45, Loss: 81.4157 +/- 21.5186, Time: 1.20\n",
      "********** Validate **********\n",
      "Epoch 45, Loss: 82.2167 +/- 24.2338, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 46, Loss: 81.2237 +/- 21.8424, Time: 1.14\n",
      "********** Validate **********\n",
      "Epoch 46, Loss: 79.1700 +/- 20.2534, Time: 0.26\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 47, Loss: 80.4388 +/- 23.4407, Time: 1.12\n",
      "********** Validate **********\n",
      "Epoch 47, Loss: 77.3787 +/- 20.7613, Time: 0.26\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 48, Loss: 80.5876 +/- 20.4491, Time: 1.16\n",
      "********** Validate **********\n",
      "Epoch 48, Loss: 81.3505 +/- 24.0468, Time: 0.30\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 49, Loss: 80.3197 +/- 21.5306, Time: 1.55\n",
      "********** Validate **********\n",
      "Epoch 49, Loss: 78.0258 +/- 20.3341, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 50, Loss: 78.6038 +/- 22.3580, Time: 1.15\n",
      "********** Validate **********\n",
      "Epoch 50, Loss: 76.6830 +/- 20.4167, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 51, Loss: 78.8208 +/- 21.1898, Time: 1.23\n",
      "********** Validate **********\n",
      "Epoch 51, Loss: 78.8453 +/- 22.8403, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 52, Loss: 77.8753 +/- 20.1148, Time: 1.16\n",
      "********** Validate **********\n",
      "Epoch 52, Loss: 74.8419 +/- 22.2515, Time: 0.26\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 53, Loss: 78.0509 +/- 22.0085, Time: 1.15\n",
      "********** Validate **********\n",
      "Epoch 53, Loss: 78.4347 +/- 19.7164, Time: 0.26\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 54, Loss: 77.2754 +/- 21.8086, Time: 1.15\n",
      "********** Validate **********\n",
      "Epoch 54, Loss: 74.7028 +/- 20.3362, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 55, Loss: 76.1301 +/- 19.3880, Time: 1.11\n",
      "********** Validate **********\n",
      "Epoch 55, Loss: 76.4519 +/- 22.7179, Time: 0.26\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 56, Loss: 76.4095 +/- 20.8888, Time: 1.13\n",
      "********** Validate **********\n",
      "Epoch 56, Loss: 73.3085 +/- 21.1656, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 57, Loss: 76.4425 +/- 21.2927, Time: 1.15\n",
      "********** Validate **********\n",
      "Epoch 57, Loss: 75.6832 +/- 19.5972, Time: 0.25\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 58, Loss: 75.6703 +/- 21.7959, Time: 1.12\n",
      "********** Validate **********\n",
      "Epoch 58, Loss: 72.6160 +/- 21.0814, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 59, Loss: 74.9942 +/- 20.1823, Time: 1.18\n",
      "********** Validate **********\n",
      "Epoch 59, Loss: 74.9521 +/- 20.7703, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 60, Loss: 74.4730 +/- 20.0872, Time: 1.11\n",
      "********** Validate **********\n",
      "Epoch 60, Loss: 73.2570 +/- 22.6745, Time: 0.24\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 61, Loss: 74.8305 +/- 20.5710, Time: 1.11\n",
      "********** Validate **********\n",
      "Epoch 61, Loss: 70.4551 +/- 20.5693, Time: 0.26\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 62, Loss: 74.9571 +/- 20.3668, Time: 1.16\n",
      "********** Validate **********\n",
      "Epoch 62, Loss: 73.4647 +/- 19.0699, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 63, Loss: 73.5664 +/- 21.8577, Time: 1.16\n",
      "********** Validate **********\n",
      "Epoch 63, Loss: 72.3368 +/- 19.9742, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 64, Loss: 73.0970 +/- 20.4515, Time: 1.14\n",
      "********** Validate **********\n",
      "Epoch 64, Loss: 70.9703 +/- 18.9295, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 65, Loss: 72.8956 +/- 20.6715, Time: 1.16\n",
      "********** Validate **********\n",
      "Epoch 65, Loss: 69.3727 +/- 20.2935, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 66, Loss: 72.1820 +/- 20.4134, Time: 1.18\n",
      "********** Validate **********\n",
      "Epoch 66, Loss: 68.8920 +/- 20.1871, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 67, Loss: 71.1098 +/- 20.1947, Time: 1.17\n",
      "********** Validate **********\n",
      "Epoch 67, Loss: 68.3536 +/- 20.0255, Time: 0.30\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 68, Loss: 71.2742 +/- 19.1738, Time: 1.18\n",
      "********** Validate **********\n",
      "Epoch 68, Loss: 68.4081 +/- 20.6071, Time: 0.26\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 69, Loss: 71.0317 +/- 19.6719, Time: 1.15\n",
      "********** Validate **********\n",
      "Epoch 69, Loss: 67.4878 +/- 20.0302, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 70, Loss: 69.9598 +/- 20.3737, Time: 1.12\n",
      "********** Validate **********\n",
      "Epoch 70, Loss: 66.9159 +/- 19.9463, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 71, Loss: 69.7548 +/- 18.8700, Time: 1.12\n",
      "********** Validate **********\n",
      "Epoch 71, Loss: 66.9515 +/- 20.1638, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 72, Loss: 69.9055 +/- 19.0000, Time: 1.12\n",
      "********** Validate **********\n",
      "Epoch 72, Loss: 65.8679 +/- 20.1273, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 73, Loss: 69.6974 +/- 19.1581, Time: 1.13\n",
      "********** Validate **********\n",
      "Epoch 73, Loss: 66.7664 +/- 19.9197, Time: 0.30\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 74, Loss: 69.6755 +/- 19.3629, Time: 1.16\n",
      "********** Validate **********\n",
      "Epoch 74, Loss: 69.2307 +/- 21.6206, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 75, Loss: 69.2106 +/- 19.3166, Time: 1.15\n",
      "********** Validate **********\n",
      "Epoch 75, Loss: 69.9512 +/- 21.5029, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 76, Loss: 69.7998 +/- 19.2330, Time: 1.15\n",
      "********** Validate **********\n",
      "Epoch 76, Loss: 65.7333 +/- 20.3489, Time: 0.32\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 77, Loss: 69.1008 +/- 19.5431, Time: 1.19\n",
      "********** Validate **********\n",
      "Epoch 77, Loss: 65.9578 +/- 18.8346, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 78, Loss: 68.4490 +/- 19.3649, Time: 1.14\n",
      "********** Validate **********\n",
      "Epoch 78, Loss: 69.9787 +/- 18.2199, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 79, Loss: 68.5626 +/- 18.4814, Time: 1.13\n",
      "********** Validate **********\n",
      "Epoch 79, Loss: 66.5947 +/- 17.7660, Time: 0.25\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 80, Loss: 69.0604 +/- 19.5953, Time: 1.17\n",
      "********** Validate **********\n",
      "Epoch 80, Loss: 66.0325 +/- 20.2752, Time: 0.30\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 81, Loss: 67.1159 +/- 17.8620, Time: 1.13\n",
      "********** Validate **********\n",
      "Epoch 81, Loss: 67.1383 +/- 19.9727, Time: 0.30\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 82, Loss: 69.2723 +/- 18.3224, Time: 1.26\n",
      "********** Validate **********\n",
      "Epoch 82, Loss: 67.7063 +/- 18.0302, Time: 0.32\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 83, Loss: 67.5017 +/- 20.2868, Time: 1.39\n",
      "********** Validate **********\n",
      "Epoch 83, Loss: 64.5396 +/- 17.6802, Time: 0.31\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 84, Loss: 67.7650 +/- 18.3294, Time: 1.32\n",
      "********** Validate **********\n",
      "Epoch 84, Loss: 67.0243 +/- 20.5738, Time: 0.34\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 85, Loss: 67.0416 +/- 18.3785, Time: 1.54\n",
      "********** Validate **********\n",
      "Epoch 85, Loss: 63.3029 +/- 17.3011, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 86, Loss: 65.4427 +/- 17.7660, Time: 1.23\n",
      "********** Validate **********\n",
      "Epoch 86, Loss: 65.0072 +/- 17.1875, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 87, Loss: 65.1729 +/- 18.7704, Time: 1.22\n",
      "********** Validate **********\n",
      "Epoch 87, Loss: 61.2745 +/- 18.3175, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 88, Loss: 63.9899 +/- 17.4897, Time: 1.21\n",
      "********** Validate **********\n",
      "Epoch 88, Loss: 62.8170 +/- 18.3579, Time: 0.32\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 89, Loss: 63.3370 +/- 16.7772, Time: 1.29\n",
      "********** Validate **********\n",
      "Epoch 89, Loss: 61.4058 +/- 18.1564, Time: 0.34\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 90, Loss: 63.7964 +/- 16.3523, Time: 1.35\n",
      "********** Validate **********\n",
      "Epoch 90, Loss: 61.4937 +/- 16.7694, Time: 0.43\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 91, Loss: 64.3158 +/- 17.4513, Time: 1.32\n",
      "********** Validate **********\n",
      "Epoch 91, Loss: 66.4220 +/- 17.3088, Time: 0.26\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 92, Loss: 64.1469 +/- 17.8281, Time: 1.22\n",
      "********** Validate **********\n",
      "Epoch 92, Loss: 61.4204 +/- 17.0619, Time: 0.36\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 93, Loss: 64.9287 +/- 17.8310, Time: 1.39\n",
      "********** Validate **********\n",
      "Epoch 93, Loss: 62.8169 +/- 18.5753, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 94, Loss: 63.3313 +/- 16.6764, Time: 1.29\n",
      "********** Validate **********\n",
      "Epoch 94, Loss: 62.0051 +/- 18.4524, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 95, Loss: 64.0707 +/- 17.9908, Time: 1.25\n",
      "********** Validate **********\n",
      "Epoch 95, Loss: 63.1209 +/- 17.1861, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 96, Loss: 63.2499 +/- 17.1438, Time: 1.35\n",
      "********** Validate **********\n",
      "Epoch 96, Loss: 63.2769 +/- 16.7446, Time: 0.34\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 97, Loss: 62.2147 +/- 17.2907, Time: 1.14\n",
      "********** Validate **********\n",
      "Epoch 97, Loss: 61.4439 +/- 17.9630, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 98, Loss: 62.2713 +/- 16.5946, Time: 1.11\n",
      "********** Validate **********\n",
      "Epoch 98, Loss: 61.0462 +/- 17.6159, Time: 0.29\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 99, Loss: 61.3214 +/- 16.9412, Time: 1.29\n",
      "********** Validate **********\n",
      "Epoch 99, Loss: 60.5157 +/- 17.3654, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 100, Loss: 61.0588 +/- 16.0362, Time: 1.18\n",
      "********** Validate **********\n",
      "Epoch 100, Loss: 60.0463 +/- 16.5751, Time: 0.30\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 101, Loss: 60.6506 +/- 16.6223, Time: 1.11\n",
      "********** Validate **********\n",
      "Epoch 101, Loss: 59.3359 +/- 16.5010, Time: 0.27\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 102, Loss: 60.0789 +/- 15.7571, Time: 1.21\n",
      "********** Validate **********\n",
      "Epoch 102, Loss: 59.1512 +/- 16.3336, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 103, Loss: 60.2831 +/- 16.4248, Time: 1.16\n",
      "********** Validate **********\n",
      "Epoch 103, Loss: 59.8530 +/- 15.7977, Time: 0.32\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 104, Loss: 59.4020 +/- 16.4468, Time: 1.24\n",
      "********** Validate **********\n",
      "Epoch 104, Loss: 58.7791 +/- 16.2918, Time: 0.33\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 105, Loss: 59.5130 +/- 16.2402, Time: 1.22\n",
      "********** Validate **********\n",
      "Epoch 105, Loss: 58.6984 +/- 16.5072, Time: 0.26\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 106, Loss: 59.0859 +/- 17.5375, Time: 1.28\n",
      "********** Validate **********\n",
      "Epoch 106, Loss: 58.4461 +/- 16.0456, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 107, Loss: 58.3751 +/- 15.9021, Time: 1.07\n",
      "********** Validate **********\n",
      "Epoch 107, Loss: 58.1264 +/- 16.1531, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 108, Loss: 58.2537 +/- 17.7103, Time: 1.10\n",
      "********** Validate **********\n",
      "Epoch 108, Loss: 58.0026 +/- 16.9824, Time: 0.32\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 109, Loss: 58.1100 +/- 15.8836, Time: 1.19\n",
      "********** Validate **********\n",
      "Epoch 109, Loss: 57.5176 +/- 16.5018, Time: 0.30\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 110, Loss: 57.7077 +/- 16.0576, Time: 1.07\n",
      "********** Validate **********\n",
      "Epoch 110, Loss: 57.9326 +/- 16.1501, Time: 0.30\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 111, Loss: 57.4776 +/- 14.4740, Time: 1.12\n",
      "********** Validate **********\n",
      "Epoch 111, Loss: 57.6871 +/- 16.5030, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 112, Loss: 57.8768 +/- 15.1143, Time: 1.15\n",
      "********** Validate **********\n",
      "Epoch 112, Loss: 57.5573 +/- 16.5601, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 113, Loss: 58.3810 +/- 16.4215, Time: 1.28\n",
      "********** Validate **********\n",
      "Epoch 113, Loss: 58.6029 +/- 16.3145, Time: 0.28\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 114, Loss: 58.8231 +/- 16.1495, Time: 1.26\n",
      "********** Validate **********\n",
      "Epoch 114, Loss: 57.1686 +/- 17.1727, Time: 0.26\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 115, Loss: 58.1907 +/- 15.8423, Time: 1.30\n",
      "********** Validate **********\n",
      "Epoch 115, Loss: 56.9879 +/- 16.6389, Time: 0.34\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 116, Loss: 58.0833 +/- 16.0107, Time: 1.25\n",
      "********** Validate **********\n",
      "Epoch 116, Loss: 58.1789 +/- 15.6460, Time: 0.32\n",
      "\n",
      "#################### Train ####################\n",
      "Epoch 117, Loss: 58.5572 +/- 16.1738, Time: 1.58\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = [], []\n",
    "for epoch in range(args['epoch_num']):\n",
    "  \n",
    "  # Train\n",
    "  train_losses.append(train(train_loader, net, epoch))\n",
    "  \n",
    "  # Validate\n",
    "  test_losses.append(validate(test_loader, net, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Xtest \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([tup[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tup \u001b[38;5;129;01min\u001b[39;00m test_set])\n\u001b[1;32m      2\u001b[0m Xtest \u001b[38;5;241m=\u001b[39m Xtest\u001b[38;5;241m.\u001b[39mto(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m ytest \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([tup[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tup \u001b[38;5;129;01min\u001b[39;00m test_set])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "Xtest = torch.stack([tup[0] for tup in test_set])\n",
    "Xtest = Xtest.to(args['device'])\n",
    "\n",
    "ytest = torch.stack([tup[1] for tup in test_set])\n",
    "ypred = net(Xtest).cpu().data\n",
    "\n",
    "data = torch.cat((ytest, ypred), axis=1)\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(data, columns=['ypred', 'ytest'])\n",
    "df_results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 9))\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(test_losses, label='Test', linewidth=3, alpha=0.5)\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.title('Convergence', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
